import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI
import re
import dotenv
import cohere
import os
from pinecone import Pinecone
import os
from langchain_openai import OpenAIEmbeddings

from streamlit_extras.colored_header import colored_header
from streamlit_extras.customize_running import center_running
from streamlit_extras.mention import mention
import streamlit as st
from streamlit_extras.stateful_chat import chat
from streamlit_extras.stodo import to_do
import random
from streamlit_extras.switch_page_button import switch_page


import re
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import pandas as pd

import dotenv
dotenv.load_dotenv()

# os.environ["http_proxy"] = "127.0.0.1:9788"
# os.environ["https_proxy"] = "127.0.0.1:9788"

# app config
st.set_page_config(page_title="æ–‡çŒ®æŠ•ç¨¿ç³»ç»Ÿ", page_icon="ğŸ¤–", layout="wide")

pc = Pinecone()


embed_model = "text-embedding-3-large"
embeddings = OpenAIEmbeddings(model=embed_model)


def is_valid_string(s):
    if s is not None and s.strip():
        # åˆ é™¤æ‰€æœ‰ç©ºæ ¼å’Œæ— æ„ä¹‰çš„å­—ç¬¦ï¼Œä¾‹å¦‚åˆ¶è¡¨ç¬¦
        cleaned_string = re.sub(r'[ \t]+', '', s)
        # æ£€æŸ¥æ¸…æ´—åçš„å­—ç¬¦ä¸²é•¿åº¦æ˜¯å¦è‡³å°‘ä¸º8
        if len(cleaned_string) >= 5:
            return s
        else:
            return ''
    return ''




df = pd.read_csv('complete_test.csv',encoding='GBK')
print(df)
df = df.rename(columns={'Cites': 'å¼•ç”¨æ•°'})
df = df.rename(columns={'Authors': 'ä½œè€…'})
df = df.rename(columns={'Title': 'æ ‡é¢˜'})
df = df.rename(columns={'Year': 'å¹´ä»½'})
df = df.rename(columns={'Source': 'æœŸåˆŠ'})
df = df.rename(columns={'ArticleURL': 'æ–‡ç« é“¾æ¥'})
df = df.rename(columns={'CitesURL': 'å¼•ç”¨é“¾æ¥'})
df = df.rename(columns={'Abstract': 'ç¼©ç•¥æ‘˜è¦'})
# åµŒå…¥å¹¶å­˜å…¥å‘é‡æ•°æ®åº“ç”Ÿæˆæ£€ç´¢å™¨
# db = Chroma.from_documents(chunks, OpenAIEmbeddings(disallowed_special=()), persist_directory=CHROMA_PATH,
#                            collection_name='paper_scan')

def filter_by_score(d,threshold):
    return d["score"] > threshold


def process_input(query: str, top_k: int, parameter_value=0.65):
    index = pc.Index('tougaoabs')
    # # raw_paths = [r"{path}".format(path=p) for p in filter_metadata]
    # # print(raw_paths)
    # # å¯¹list2ä¸­çš„å­—ç¬¦ä¸²è¿›è¡Œè½¬æ¢
    # unescaped_list2 = [unescape_string(s) for s in filter_metadata]

    # å¯¹æŸ¥è¯¢è¿›è¡Œç¼–ç 
    xq = embeddings.embed_query(query)
    # åœ¨ Pinecone ç´¢å¼•ä¸­æœç´¢
    q = index.query(vector=xq, top_k=top_k,
                    include_metadata=True
                    )
    # print('q:',len(q['matches']))
    # print('q score:', q['matches'][0]['score'])
    # ä½¿ç”¨ lambda å‡½æ•°æ¥ä¼ é€’é¢å¤–çš„ threshold å‚æ•°
    docs = list(filter(lambda d: filter_by_score(d, parameter_value), q['matches']))

    print('é´é€‰å¾—åˆ°æœ€åçš„æ–‡æ¡£æ•°',len(docs))
    # print(docs)
    num_docs = len(docs)
    # print(list(docs[1].metadata.values())[0])

    row_index = []

    for i in range(len(docs)):
        row_index.append(list(docs[i].metadata.values())[0])
    print(row_index)
    result = df.loc[row_index]
    # print(result)
    # print(result.iloc[3])
    second_para = ''
    for i in range(len(row_index)):
        test = result.iloc[i].to_json()
        print(test)
        json_str_trimmed = test[1:-1]
        # å°†é€—å·æ›¿æ¢ä¸ºé€—å·åŠ æ¢è¡Œç¬¦
        json_str_with_newlines = json_str_trimmed.replace(",", ",\n")
        # print(json_str_with_newlines)
        print(json_str_with_newlines)
        # æ‰¾åˆ° "ä½œè€…" å­—æ®µçš„èµ·å§‹å’Œç»“æŸä½ç½®
        start = json_str_with_newlines.find("\\u4f5c")
        end = json_str_with_newlines.find("\\u6807", start)
        print(start,end)

        # æ£€æŸ¥ "ä½œè€…" å­—æ®µä¸­æ˜¯å¦åŒ…å«æ¢è¡Œç¬¦ï¼Œå¹¶è¿›è¡Œç›¸åº”çš„å¤„ç†
        if start != -1 and end != -1 and '\n' in json_str_with_newlines[start:end]:
            # ä¿ç•™æœ€åä¸€ä¸ªæ¢è¡Œç¬¦ï¼Œå…¶ä»–çš„æ¢è¡Œç¬¦æ›¿æ¢ä¸ºé€—å·
            authors_part = json_str_with_newlines[start:end].rsplit('\n', 1)
            authors_part[0] = authors_part[0].replace('\n', ', ')
            json_str_with_newlines = str(i+1) + '.' + ' ' + json_str_with_newlines[:start] + ''.join(authors_part) + '\n' + json_str_with_newlines[end:] + '\n\n'   # str(i+1) + 'ã€' +
    # print(json_str_with_newlines)
            chinese_str = json_str_with_newlines.encode().decode('unicode_escape')
            second_para += chinese_str

    print('è®ºæ–‡å†…å®¹ï¼š', second_para)
    result_json = result.to_json()
    # print(result_json)
    # ç¬¬2æ­¥ï¼šåœ¨resultä¸­å¤„ç† "Source" å­—æ®µ
    source_counts_result = result['æœŸåˆŠ'].value_counts()
    # ç¬¬3æ­¥ï¼šåœ¨åŸå§‹DataFrameä¸­ç»Ÿè®¡æ¯ä¸ª "Source" å…ƒç´ çš„è¡Œæ•°
    source_counts_original = df['æœŸåˆŠ'].value_counts()

    # ç¬¬4æ­¥ï¼šè®¡ç®—æ¯”ä¾‹
    ratios = source_counts_result / source_counts_original

    # å¯¹æ¯”ä¾‹è¿›è¡Œæ’åºå¹¶ç§»é™¤0æˆ–NaNå€¼
    ratios_sorted = ratios.sort_values(ascending=False).dropna()
    ratios_sorted = ratios_sorted[ratios_sorted > 0]

    # # ç¬¬5æ­¥ï¼šæ‰“å°ç»“æœ
    # print("æŒ‰ç›¸å…³æ¯”ä¾‹æ’åºçš„æœŸåˆŠæ¬¡åºä¸ºï¼š")
    # print(ratios_sorted)
    # print("æŒ‰ç»å¯¹å€¼æ’åºçš„æœŸåˆŠæ¬¡åºä¸ºï¼š")
    # print(source_counts_result)

    print(ratios_sorted.to_json())
    # ç„¶åæ›¿æ¢é€—å·ï¼ˆ,ï¼‰ä¸ºé€—å·åŠ æ¢è¡Œç¬¦ï¼ˆ,\nï¼‰
    json_str = ratios_sorted.to_json()
    # å»æ‰é¦–å°¾çš„ {} ç¬¦å·
    json_str_trimmed = json_str[1:-1]
    # å°†é€—å·æ›¿æ¢ä¸ºé€—å·åŠ æ¢è¡Œç¬¦
    json_str_with_newlines = json_str_trimmed.replace(",", ",\n")
    # å°†åŒå¼•å·æ›¿æ¢ä¸ºä¹¦åå·
    rate_journal_sort = re.sub(r'"([^"]+)"', r'ã€Š\1ã€‹', json_str_with_newlines)
    cleaned_str1 = rate_journal_sort.replace('\\u2026', '')
    print(cleaned_str1)

    # ç„¶åæ›¿æ¢é€—å·ï¼ˆ,ï¼‰ä¸ºé€—å·åŠ æ¢è¡Œç¬¦ï¼ˆ,\nï¼‰
    json_str2 = source_counts_result.to_json()
    # å»æ‰é¦–å°¾çš„ {} ç¬¦å·
    json_str_trimmed2 = json_str2[1:-1]
    # å°†é€—å·æ›¿æ¢ä¸ºé€—å·åŠ æ¢è¡Œç¬¦
    json_str_with_newlines2 = json_str_trimmed2.replace(",", ",\n")
    # å°†åŒå¼•å·æ›¿æ¢ä¸ºä¹¦åå·
    jueduizhi_sort_journal = re.sub(r'"([^"]+)"', r'ã€Š\1ã€‹', json_str_with_newlines2)
    cleaned_str = jueduizhi_sort_journal.replace('\\u2026', '')
    print(cleaned_str)

    first_para = f'æ ¹æ®æ‚¨çš„è®ºæ–‡å†…å®¹ä¸è®¾å®šçš„ç›¸å…³æ€§é˜ˆå€¼({parameter_value})ï¼Œå·²åœ¨ABSæœŸåˆŠèŒƒå›´å†…ä¸ºæ‚¨æ‰¾åˆ°{num_docs}ç¯‡ç›¸å…³è®ºæ–‡å¦‚ä¸‹ï¼š\n'
    third_para = f'\nä¸‹é¢æ˜¯ç›¸å…³è®ºæ–‡åœ¨å„æœŸåˆŠä¸­çš„åˆ†å¸ƒè®¡ç®—ï¼Œé¦–å…ˆä»æ•°é‡åˆ†å¸ƒä¸Šæ¥çœ‹ï¼Œç»“æœå¦‚ä¸‹ï¼š\n\n'
    fourth_para = cleaned_str
    fifth_para = f'\n\nç„¶åä»æ¯”ä¾‹åˆ†å¸ƒä¸Šæ¥çœ‹ï¼Œç»“æœå¦‚ä¸‹ï¼š\n\n'
    sixth_para = cleaned_str1

    return [first_para,second_para,third_para,fourth_para,fifth_para,sixth_para]




print('ç¨‹åºå¯åŠ¨å®Œæˆ')




coltougao_1, coltougao_2 = st.columns([2.5, 1])

with coltougao_2:

    colored_header(
        label="å‚æ•°é…ç½®",
        description="",
        color_name="blue-70",
    )


    # å¤§æ¨¡å‹çš„æ¸©åº¦é€‰æ‹©ï¼Œè¿”å›é‡æ’åºä¹‹åçš„chunks
    similarity_value = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼è®¾ç½®ï¼ˆè¿‡æ»¤ä½äºè¯¥å€¼çš„è®ºæ–‡ï¼‰", min_value=0.2, max_value=0.7, value=0.4, step=0.05,
                           key="slider2_tougao")


    chunks_limit = st.slider("æœ€å¤šè¿”å›çš„è®ºæ–‡æ•°ï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºï¼‰", min_value=5, max_value=50, value=20, step=1,
                           key="slider3_tougao")

with coltougao_1:
    colored_header(
        label="è®ºæ–‡æŠ•ç¨¿æ¨èç³»ç»Ÿï¼ˆæš‚åªåŒ…æ‹¬ABSæœŸåˆŠä¸­æ•°åç§ä¸æµåŠ¨æ€§ç ”ç©¶æœ‰å…³çš„æœŸåˆŠï¼‰",
        description="",
        color_name="violet-70",
    )
    # session state
    if "tougao_history" not in st.session_state:
        st.session_state.tougao_history = [
            AIMessage(content="ä½ å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„æŠ•ç¨¿æ¨èåŠ©æ‰‹ï¼Œè¯·å°†æ‚¨éœ€è¦æŠ•ç¨¿è®ºæ–‡çš„æ ‡é¢˜æˆ–è€…æ‘˜è¦ç­‰å†…å®¹ç›´æ¥å‘Šè¯‰æˆ‘ï¼Œæˆ‘å°†ä¸ºä½ æ¨èç›¸å…³è®ºæ–‡å’ŒæœŸåˆŠ~"),
        ]

    # è®¾ç½®ä¸€ä¸ªæŒ‰é’®ï¼Œç”¨äºæ¸…ç©ºé™¤äº†ç¬¬ä¸€æ¡æ¶ˆæ¯ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯
    if st.button(label="æ¸…ç©ºå¯¹è¯è®°å½•",type="primary"):
        st.session_state.tougao_history = st.session_state.tougao_history[:1]



    # conversation
    for message in st.session_state.tougao_history:
        if isinstance(message, AIMessage):
            with st.chat_message("AI"):
                st.write(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("Human"):
                st.write(message.content)

    # åˆ›å»ºç”¨æˆ·è¾“å…¥çš„å ä½ç¬¦
    # # åˆ›å»ºä¸€ä¸ªå®¹å™¨æ¥æŒç»­æ›´æ–°å†…å®¹
    # content = st.container()
    #
    # # åœ¨è„šæœ¬çš„æœ«å°¾åˆ›å»ºä¸€ä¸ªè¾“å…¥æ¡†å ä½ç¬¦
    # input_placeholder = st.empty()
    # user input
user_query = st.chat_input("Type your message here...")

with coltougao_1:
    if user_query is not None and user_query != "":
        st.session_state.tougao_history.append(HumanMessage(content=user_query))

        with st.chat_message("Human"):
            st.markdown(user_query)

        with st.chat_message("AI"):
            response = process_input(query=user_query, parameter_value=similarity_value, top_k=chunks_limit)
            response_str = "\n".join(response)  # å°†åˆ—è¡¨è¿æ¥æˆä¸€ä¸ªå•ç‹¬çš„å­—ç¬¦ä¸²
            print(response_str)
            st.text(response_str)

        st.session_state.tougao_history.append(AIMessage(content=response_str))







