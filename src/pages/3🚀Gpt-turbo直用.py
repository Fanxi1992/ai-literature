import streamlit as st
from langchain_core.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
# import pinecone
from p_version_module_pinecone_doc_name import get_doc_names
from p_version_module_process_new_papers import process_files
from p_version_module_return_call_chunks import return_chunks
from tempfile import NamedTemporaryFile
import re
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate

from streamlit_extras.colored_header import colored_header
from streamlit_extras.customize_running import center_running
from streamlit_extras.mention import mention
import streamlit as st
from streamlit_extras.stateful_chat import chat
from streamlit_extras.stodo import to_do
import random
from streamlit_extras.switch_page_button import switch_page
import dotenv
dotenv.load_dotenv()


# app config
st.set_page_config(page_title="Streamlit Chatbot", page_icon="ğŸ¤–", layout="wide")


def is_valid_string(s):
    if s is not None and s.strip():
        # åˆ é™¤æ‰€æœ‰ç©ºæ ¼å’Œæ— æ„ä¹‰çš„å­—ç¬¦ï¼Œä¾‹å¦‚åˆ¶è¡¨ç¬¦
        cleaned_string = re.sub(r'[ \t]+', '', s)
        # æ£€æŸ¥æ¸…æ´—åçš„å­—ç¬¦ä¸²é•¿åº¦æ˜¯å¦è‡³å°‘ä¸º8
        if len(cleaned_string) >= 5:
            return s
        else:
            return ''
    return ''

def get_response(temperature, system_prompt, user_query, turbo_history):
    template = f"""
    You are a helpful assistant. 
    {system_prompt}
    Answer the following questions considering the history of the conversation:

    Chat history: {{turbo_history}}

    User question: {{user_question}}
    """

    prompt = ChatPromptTemplate.from_template(template)

    llm = ChatOpenAI(
        model='gpt-4-turbo-2024-04-09',
        temperature=temperature,
        streaming=True,
        openai_api_base="https://api.gptsapi.net/v1"
    )

    chain = prompt | llm | StrOutputParser()

    return chain.stream({
        "turbo_history": turbo_history,
        "user_question": user_query,
    })


col11, col22 = st.columns([2.5, 1])

with col22:

    colored_header(
        label="å‚æ•°é…ç½®",
        description="",
        color_name="blue-70",
    )


    # å¤§æ¨¡å‹çš„æ¸©åº¦é€‰æ‹©ï¼Œè¿”å›é‡æ’åºä¹‹åçš„chunks
    temperature = st.slider("å¤§æ¨¡å‹æ¸©åº¦é€‰æ‹©ï¼ˆæ•°å­—è¶Šå¤§è¶Šå¤šæ ·ï¼Œè¶Šå°è¶Šä¸¥è°¨ï¼‰", min_value=0.0, max_value=1.0, value=0.2, step=0.1,
                           key="slider2")


    system_prompt = st.text_area("ç³»ç»Ÿæç¤ºï¼ˆç»™å¤§æ¨¡å‹çš„é¢„è®¾è§„èŒƒ,è¾“å‡ºåˆ™ç”Ÿæ•ˆï¼‰", value="", height=200, max_chars=1000, key="system_prompt2")
    print(system_prompt)

with col11:
    colored_header(
        label="My Chatbot",
        description="",
        color_name="violet-70",
    )
    # session state
    if "turbo_history" not in st.session_state:
        st.session_state.turbo_history = [
            AIMessage(content="ä½ å¥½ï¼Œæˆ‘æ˜¯å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹gpt-4-turbo-2024-04-09ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ"),
        ]

    # è®¾ç½®ä¸€ä¸ªæŒ‰é’®ï¼Œç”¨äºæ¸…ç©ºé™¤äº†ç¬¬ä¸€æ¡æ¶ˆæ¯ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯
    if st.button(label="æ¸…ç©ºå¯¹è¯è®°å½•",type="primary"):
        st.session_state.turbo_history = st.session_state.turbo_history[:1]


    system_regulate = is_valid_string(system_prompt)

    # conversation
    for message in st.session_state.turbo_history:
        if isinstance(message, AIMessage):
            with st.chat_message("AI"):
                st.write(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("Human"):
                st.write(message.content)

    # åˆ›å»ºç”¨æˆ·è¾“å…¥çš„å ä½ç¬¦
    # # åˆ›å»ºä¸€ä¸ªå®¹å™¨æ¥æŒç»­æ›´æ–°å†…å®¹
    # content = st.container()
    #
    # # åœ¨è„šæœ¬çš„æœ«å°¾åˆ›å»ºä¸€ä¸ªè¾“å…¥æ¡†å ä½ç¬¦
    # input_placeholder = st.empty()
    # user input
user_query = st.chat_input("Type your message here...")

with col11:
    if user_query is not None and user_query != "":
        st.session_state.turbo_history.append(HumanMessage(content=user_query))

        with st.chat_message("Human"):
            st.markdown(user_query)

        with st.chat_message("AI"):
            response = st.write_stream(get_response(temperature, system_regulate, user_query, st.session_state.turbo_history))

        st.session_state.turbo_history.append(AIMessage(content=response))








